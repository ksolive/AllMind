# 虚拟人

直接使用assistent实现有很大难度。实际实现中发现大部分逻辑还是程序实现的，所以现在重新构建框架。

仿生结构，以输入输出划分

输入：

- [ ] 文字
- [ ] 图像
- [ ] 语音
- [ ] 视频
- [ ] 系统操作

输出：

- [ ] 文字
- [ ] 图像
- [ ] 语音
- [ ] 系统操作
- [ ] live2d动作

所有输入单独并行运行，作用是将对应输入形式统一转化为文字描述。

    e.g. <语音> => <文字描述>

所有文字输入描述传递给汇总总结助手，该助手定时总结收到的文字描述，赋予其时间属性。

    e.g. 用户点开了xxx网页，输入了xxx内容，2秒后说xxx，然后对你说xxx，请回复

同时该助手起到了假主动对话发起器的作用，因为他会在一定时间后自动调用虚拟人。

当用户主动发起对话是该助手也会立即进行总结，主动发起对话可能包括

- [ ] 用户点击了虚拟人
- [ ] 用户向虚拟人提问
- [ ] 用户语音发起提问

虚拟人获取对话后判断需要以哪些形式输出。一般而言都需要输出文字，并且经过tts转化为语音，然后输出控制live2d动作的指令。而生成图像以及进行系统操作则是不一定进行的操作。

以assistant的经验，让ai自行判断可能并行的输出形式是困难的。因此或许可以将判断分离出来，比如让ai专注啊于判断是否需要画画之类，但这样会极大提高token消耗。

## 可能的拓展

记忆模块，但不知道应该加在哪里。